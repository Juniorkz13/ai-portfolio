
# ğŸš€ Customer Churn Prediction API â€” MLOps & Deployment

This project demonstrates an end-to-end **Machine Learning deployment pipeline**, covering
model training, preprocessing, inference, and containerized deployment using **FastAPI** and **Docker**.

The objective is to expose a **production-ready churn prediction service** with a clear
feature contract, robust preprocessing, and versioned inference endpoints.

---

## ğŸ§  Project Overview

Customer churn prediction is a critical business problem for subscription-based companies.
In this project, a trained Machine Learning model is deployed as a REST API, enabling
real-time churn prediction from structured customer data.

This project focuses not only on modeling, but also on **production concerns**, such as:
- Feature schema alignment
- Robust preprocessing
- Inference reliability
- Deployment reproducibility

---

## ğŸ—ï¸ Architecture

```
Client (JSON Request)
        â†“
FastAPI (/v1/predict)
        â†“
Pydantic Validation
        â†“
Pandas DataFrame
        â†“
Preprocessing Pipeline (Scaling + One-Hot Encoding)
        â†“
Trained ML Model
        â†“
Prediction Response (JSON)
```

---

## ğŸ“¦ Tech Stack

- Python 3.10
- FastAPI
- Scikit-learn
- Pandas
- Joblib
- Docker
- Docker Compose

---

## ğŸ“ Project Structure

```
projects/mlops_deploy/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI application
â”‚   â”œâ”€â”€ schemas.py       # Request/response schemas
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ churn_pipeline_v1.joblib
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## â–¶ï¸ Running the API Locally

From the root of the repository:

```bash
uvicorn projects.mlops_deploy.app.main:app --reload
```

Open:
- Swagger UI: http://127.0.0.1:8000/docs

---

## ğŸ³ Running with Docker

### Build and run manually
```bash
docker build -f projects/mlops_deploy/Dockerfile -t churn-api projects/mlops_deploy
docker run -p 8000:8000 churn-api
```

### Using Docker Compose
```bash
docker compose up --build
```

---

## ğŸ”® Prediction Endpoint

**POST** `/v1/predict`

### Example Request
```json
{
  "tenure": 12,
  "monthly_charges": 75.5,
  "total_charges": 900.0,
  "contract_type": "Month-to-month",
  "payment_method": "Electronic check",
  "internet_service": "Fiber optic"
}
```

### Example Response
```json
{
  "churn_probability": 0.8396,
  "churn_prediction": 1
}
```

---

## ğŸ“Œ MLOps Highlights

- Explicit feature contract between training and inference
- Robust handling of unseen categorical values
- Versioned API endpoint (`/v1/predict`)
- Separation of training and serving logic
- Fully containerized and reproducible deployment

---

## ğŸš€ Future Improvements

- Model monitoring and drift detection
- Advanced model versioning
- CI/CD pipeline
- Cloud deployment (AWS, GCP, Azure)

---

## ğŸ‘¤ Author

**JosÃ© Geraldo do EspÃ­rito Santo JÃºnior**  
AI & Machine Learning Portfolio  
Location: Brazil
